import streamlit as st
import pandas as pd
from openai import OpenAI
import json
from datetime import datetime
import re
from io import BytesIO
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

st.set_page_config(
    page_title="An√°lise de Risco de Externaliza√ß√£o - Base Manifesta√ß√µes",
    page_icon="‚ö†Ô∏è",
    layout="wide"
)

# Configurar OpenAI API usando secrets do Streamlit
try:
    client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
except Exception as e:
    st.error("‚ö†Ô∏è Erro ao configurar OpenAI API. Verifique se a chave est√° configurada em Settings > Secrets do Streamlit.")
    st.stop()

# Configura√ß√µes OTIMIZADAS
BATCH_SIZE = 100  # Aumentado de 50 para 100
MAX_WORKERS = 20  # N√∫mero de threads paralelas
MODEL = "gpt-4o-mini"  # Modelo mais r√°pido que gpt-4.1-mini
MAX_TOKENS_INTERNAL = 400  # Reduzido de 600
MAX_TOKENS_EXTERNAL = 500  # Reduzido de 800

def classify_internal_risk(score):
    """Classifica risco interno (0-100) de forma granular"""
    if score >= 75:
        return "üî¥ RISCO ALTO DE EXTERNALIZAR"
    else:
        lower = (score // 5) * 5
        upper = lower + 5
        if upper > 74:
            upper = 74
        return f"{lower}-{upper} pts"

def classify_external_risk(score):
    """Classifica risco externo (100-1000)"""
    if score >= 851:
        return "üî¥ Vai Reclamar Novamente"
    elif score >= 701:
        return "üü† Muito Alto"
    elif score >= 501:
        return "üü° Alto"
    elif score >= 301:
        return "üü¢ M√©dio"
    else:
        return "‚ö™ Baixo"

def classify_channel_type(channel_value):
    """Classifica o canal como Interno ou Externo"""
    if pd.isna(channel_value):
        return "Interno", 0
    
    channel_str = str(channel_value).strip().lower()
    
    # Externos
    if "ouvidoria" in channel_str:
        return "Externo", 100
    elif "reclame aqui" in channel_str or "reclameaqui" in channel_str:
        return "Externo", 75
    elif "focais" in channel_str or "externo - focais" in channel_str:
        return "Externo", 50
    elif "externo" in channel_str:
        return "Externo", 75
    
    # Internos
    else:
        return "Interno", 0

def analyze_internal_risk(client, text, nr_ocorrencia="N/A"):
    """EIXO 1: An√°lise de risco de reclama√ß√µes INTERNAS virarem EXTERNAS (0-100 pontos)"""
    
    prompt = f"""Voc√™ √© um analista preditivo especializado em prever o risco de reclama√ß√µes internas se tornarem externas.

CONTEXTO: Reclama√ß√£o INTERNA (NR_OCORRENCIA: {nr_ocorrencia})

TEXTO: {text[:1500]}

TAREFA: Calcule o risco (0-100 pontos) de esta reclama√ß√£o INTERNA se tornar EXTERNA.

METODOLOGIA:
1. FREQU√äNCIA DE CONTATOS (Peso 4, m√°x 40 pts)
2. TEMPO DE ESPERA (Peso 3, m√°x 30 pts)
3. FALHAS OPERACIONAIS (Peso 2, m√°x 20 pts)
4. ESTADO EMOCIONAL (Peso 1, m√°x 10 pts)

REGRA: Negativas t√©cnicas sem insatisfa√ß√£o = m√°ximo 30 pontos

FORMATO JSON:
{{"risk_score": <0-100>, "frequency_score": <0-40>, "delay_score": <0-30>, "operational_score": <0-20>, "emotional_score": <0-10>, "key_factors": ["fator1"], "detected_threats": ["amea√ßa1"], "emotional_tone": "descri√ß√£o", "is_technical_negative": true/false, "recommendation": "a√ß√£o"}}

Retorne APENAS o JSON."""

    try:
        response = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": "Voc√™ √© um analista preditivo especializado."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,
            max_tokens=MAX_TOKENS_INTERNAL
        )
        
        result_text = response.choices[0].message.content.strip()
        json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
        
        if json_match:
            return json.loads(json_match.group())
        else:
            return {"risk_score": 0, "frequency_score": 0, "delay_score": 0, "operational_score": 0, "emotional_score": 0, "key_factors": ["Erro"], "detected_threats": [], "emotional_tone": "N/A", "is_technical_negative": False, "recommendation": "Revisar"}
            
    except Exception as e:
        return {"risk_score": 0, "frequency_score": 0, "delay_score": 0, "operational_score": 0, "emotional_score": 0, "key_factors": [str(e)[:50]], "detected_threats": [], "emotional_tone": "N/A", "is_technical_negative": False, "recommendation": "Erro"}

def analyze_external_risk(client, text, nr_ocorrencia="N/A", channel_base_score=50):
    """EIXO 2: An√°lise de risco de reclama√ß√µes EXTERNAS serem ESCALADAS/REPETIDAS (100-1000 pontos)"""
    
    prompt = f"""Voc√™ √© um analista preditivo especializado em prever escala√ß√£o de reclama√ß√µes externas.

CONTEXTO: Reclama√ß√£o EXTERNA (NR_OCORRENCIA: {nr_ocorrencia}) | Peso base: {channel_base_score} pts

TEXTO: {text[:1500]}

TAREFA: Calcule o risco (100-1000 pontos) de o cliente ESCALAR ou RECLAMAR NOVAMENTE.

METODOLOGIA:
1. INDICADORES TEXTUAIS (Peso 5, m√°x 500 pts)
2. INSATISFA√á√ÉO ANTERIOR (Peso 3, m√°x 300 pts)
3. GRAVIDADE DO CANAL (Peso 2, m√°x 200 pts)

FORMATO JSON:
{{"risk_score": <100-1000>, "external_indicators_score": <0-500>, "previous_dissatisfaction_score": <0-300>, "channel_gravity_score": <0-200>, "channel_base_score": {channel_base_score}, "repeat_probability": "Baixa/M√©dia/Alta/Muito Alta/Certeza", "escalation_channels": ["canal1"], "previous_complaints_detected": true/false, "behavioral_patterns": ["padr√£o1"], "key_indicators": ["indicador1"], "urgency_level": "Baixa/M√©dia/Alta/Urgente", "recommendation": "a√ß√£o"}}

Retorne APENAS o JSON."""

    try:
        response = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": "Voc√™ √© um analista preditivo especializado."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,
            max_tokens=MAX_TOKENS_EXTERNAL
        )
        
        result_text = response.choices[0].message.content.strip()
        json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
        
        if json_match:
            result = json.loads(json_match.group())
            score = result.get("risk_score", 100)
            if score < 100:
                score = 100 + score
            result["risk_score"] = min(1000, score)
            return result
        else:
            return {"risk_score": 100 + channel_base_score, "external_indicators_score": 0, "previous_dissatisfaction_score": 0, "channel_gravity_score": 0, "channel_base_score": channel_base_score, "repeat_probability": "N/A", "escalation_channels": [], "previous_complaints_detected": False, "behavioral_patterns": [], "key_indicators": ["Erro"], "urgency_level": "N/A", "recommendation": "Revisar"}
            
    except Exception as e:
        return {"risk_score": 100 + channel_base_score, "external_indicators_score": 0, "previous_dissatisfaction_score": 0, "channel_gravity_score": 0, "channel_base_score": channel_base_score, "repeat_probability": "N/A", "escalation_channels": [], "previous_complaints_detected": False, "behavioral_patterns": [], "key_indicators": [str(e)[:50]], "urgency_level": "N/A", "recommendation": "Erro"}

def process_single_row(row_data):
    """Processa uma √∫nica linha (ser√° executada em thread separada)"""
    idx, row, channel_col, text_col, client_key = row_data
    
    try:
        # Criar cliente OpenAI local para esta thread
        local_client = OpenAI(api_key=client_key)
        
        channel_value = row[channel_col] if channel_col else None
        text_value = row[text_col] if text_col else ""
        
        channel_type, channel_base = classify_channel_type(channel_value)
        
        nr_ocorrencia = row.get('NR_OCORRENCIA', 'N/A')
        nr_pedido_wa = row.get('NR_PEDIDO_WA', 'N/A')
        tipo_manifestacao = row.get('TIPO_MANIFESTACAO', '')
        situacao = row.get('SITUACAO', '')
        
        full_text = f"N√∫mero: {nr_ocorrencia}\nPedido: {nr_pedido_wa}\nTipo: {tipo_manifestacao}\nSitua√ß√£o: {situacao}\nCanal: {channel_value}\n\nHist√≥rico: {text_value}"
        
        if channel_type == "Interno":
            # An√°lise INTERNA
            analysis = analyze_internal_risk(local_client, full_text, nr_ocorrencia)
            score = analysis.get("risk_score", 0)
            classification = classify_internal_risk(score)
            
            return {
                "Linha": idx + 1,
                "NR_OCORRENCIA": nr_ocorrencia if nr_ocorrencia != 'N/A' else '',
                "Pedido": nr_pedido_wa if nr_pedido_wa != 'N/A' else '',
                "Canal": channel_value if channel_value else '',
                "Tipo": channel_type,
                "Tipo Manifesta√ß√£o": tipo_manifestacao,
                "Situa√ß√£o": situacao,
                "Pontua√ß√£o": score,
                "Classifica√ß√£o": classification,
                "Score Frequ√™ncia": analysis.get("frequency_score", 0) if analysis.get("frequency_score") is not None else 0,
                "Score Atraso": analysis.get("delay_score", 0) if analysis.get("delay_score") is not None else 0,
                "Score Operacional": analysis.get("operational_score", 0) if analysis.get("operational_score") is not None else 0,
                "Score Emocional": analysis.get("emotional_score", 0) if analysis.get("emotional_score") is not None else 0,
                "Fatores Cr√≠ticos": ", ".join(analysis.get("key_factors", [])) if analysis.get("key_factors") else "",
                "Amea√ßas Detectadas": ", ".join(analysis.get("detected_threats", [])) if analysis.get("detected_threats") else "",
                "Tom Emocional": analysis.get("emotional_tone", "") if analysis.get("emotional_tone") and analysis.get("emotional_tone") != "N/A" else "",
                "Negativa T√©cnica?": "Sim" if analysis.get("is_technical_negative", False) else "N√£o",
                "Recomenda√ß√£o": analysis.get("recommendation", "") if analysis.get("recommendation") and analysis.get("recommendation") != "N/A" else "",
                # Campos vazios para externos (internos n√£o t√™m)
                "Score Indicadores Externos": "",
                "Score Insatisfa√ß√£o Anterior": "",
                "Score Gravidade Canal": "",
                "Peso Base Canal": "",
                "Padr√µes Comportamentais": "",
                "Canais de Escala√ß√£o": "",
                "Probabilidade Repetir": "",
                "Urg√™ncia": ""
            }
            
        else:  # Externo
            # An√°lise EXTERNA
            analysis = analyze_external_risk(local_client, full_text, nr_ocorrencia, channel_base)
            score = analysis.get("risk_score", 100)
            classification = classify_external_risk(score)
            
            return {
                "Linha": idx + 1,
                "NR_OCORRENCIA": nr_ocorrencia if nr_ocorrencia != 'N/A' else '',
                "Pedido": nr_pedido_wa if nr_pedido_wa != 'N/A' else '',
                "Canal": channel_value if channel_value else '',
                "Tipo": channel_type,
                "Tipo Manifesta√ß√£o": tipo_manifestacao,
                "Situa√ß√£o": situacao,
                "Pontua√ß√£o": score,
                "Classifica√ß√£o": classification,
                "Score Indicadores Externos": analysis.get("external_indicators_score", 0) if analysis.get("external_indicators_score") is not None else 0,
                "Score Insatisfa√ß√£o Anterior": analysis.get("previous_dissatisfaction_score", 0) if analysis.get("previous_dissatisfaction_score") is not None else 0,
                "Score Gravidade Canal": analysis.get("channel_gravity_score", 0) if analysis.get("channel_gravity_score") is not None else 0,
                "Peso Base Canal": analysis.get("channel_base_score", channel_base) if analysis.get("channel_base_score") is not None else channel_base,
                "Padr√µes Comportamentais": ", ".join(analysis.get("behavioral_patterns", [])) if analysis.get("behavioral_patterns") else "",
                "Canais de Escala√ß√£o": ", ".join(analysis.get("escalation_channels", [])) if analysis.get("escalation_channels") else "",
                "Probabilidade Repetir": analysis.get("repeat_probability", "") if analysis.get("repeat_probability") and analysis.get("repeat_probability") != "N/A" else "",
                "Urg√™ncia": analysis.get("urgency_level", "") if analysis.get("urgency_level") and analysis.get("urgency_level") != "N/A" else "",
                "Recomenda√ß√£o": analysis.get("recommendation", "") if analysis.get("recommendation") and analysis.get("recommendation") != "N/A" else "",
                # Campos vazios para internos (externos n√£o t√™m)
                "Score Frequ√™ncia": "",
                "Score Atraso": "",
                "Score Operacional": "",
                "Score Emocional": "",
                "Fatores Cr√≠ticos": ", ".join(analysis.get("key_indicators", [])) if analysis.get("key_indicators") else "",
                "Amea√ßas Detectadas": ", ".join(analysis.get("escalation_channels", [])) if analysis.get("escalation_channels") else "",
                "Tom Emocional": "",
                "Negativa T√©cnica?": ""
            }
            
    except Exception as e:
        return None

def process_batch_parallel(df_batch, channel_col, text_col, client_key):
    """Processa um lote de linhas em PARALELO usando ThreadPoolExecutor"""
    
    # Preparar dados para processamento paralelo
    row_data_list = [
        (idx, row, channel_col, text_col, client_key)
        for idx, row in df_batch.iterrows()
    ]
    
    results = []
    
    # Processar em paralelo com ThreadPoolExecutor
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # Submeter todas as tarefas
        future_to_row = {
            executor.submit(process_single_row, row_data): row_data[0]
            for row_data in row_data_list
        }
        
        # Coletar resultados conforme completam
        for future in as_completed(future_to_row):
            try:
                result = future.result()
                if result is not None:
                    results.append(result)
            except Exception as e:
                continue
    
    return results

def process_excel_in_batches_optimized(uploaded_file, client):
    """Processa o Excel em lotes com processamento paralelo OTIMIZADO"""
    try:
        df = pd.read_excel(uploaded_file, sheet_name='Base Manifesta√ß√µes')
        
        st.info(f"üìä Total de linhas: {len(df)}")
        
        col_names = df.columns.tolist()
        channel_col = col_names[30] if len(col_names) > 30 else None
        
        text_col = None
        for col in col_names:
            if 'HISTORICO' in str(col).upper() or 'MANIFESTACAO' in str(col).upper():
                text_col = col
                break
        
        if text_col is None:
            for col in df.columns:
                if df[col].dtype == 'object':
                    avg_length = df[col].astype(str).str.len().mean()
                    if avg_length > 100:
                        text_col = col
                        break
        
        # Pegar API key para passar para threads
        client_key = st.secrets["OPENAI_API_KEY"]
        
        # Dividir em lotes
        total_rows = len(df)
        num_batches = (total_rows + BATCH_SIZE - 1) // BATCH_SIZE
        
        st.info(f"üöÄ **Modo OTIMIZADO**: {num_batches} lotes de at√© {BATCH_SIZE} linhas | {MAX_WORKERS} threads paralelas")
        
        all_results = []
        progress_bar = st.progress(0)
        status_text = st.empty()
        batch_info = st.empty()
        
        start_time = time.time()
        
        for batch_num in range(num_batches):
            batch_start = batch_num * BATCH_SIZE
            batch_end = min(batch_start + BATCH_SIZE, total_rows)
            
            df_batch = df.iloc[batch_start:batch_end]
            
            batch_info.info(f"üì¶ **Lote {batch_num + 1}/{num_batches}** | Linhas {batch_start + 1} a {batch_end} de {total_rows} | ‚ö° Processamento Paralelo")
            
            batch_start_time = time.time()
            
            # Processar lote de forma PARALELA
            batch_results = process_batch_parallel(df_batch, channel_col, text_col, client_key)
            all_results.extend(batch_results)
            
            batch_time = time.time() - batch_start_time
            
            # Atualizar progresso
            progress = (batch_num + 1) / num_batches
            progress_bar.progress(progress)
            
            # Calcular tempo restante
            if batch_num < num_batches - 1:
                avg_batch_time = (time.time() - start_time) / (batch_num + 1)
                remaining_batches = num_batches - (batch_num + 1)
                estimated_seconds = remaining_batches * avg_batch_time
                estimated_minutes = int(estimated_seconds / 60)
                
                status_msg = f"‚úÖ Lote {batch_num + 1}/{num_batches} conclu√≠do em {batch_time:.1f}s"
                status_msg += f" | ‚ö° {len(df_batch)/(batch_time+0.001):.1f} linhas/seg"
                
                if estimated_minutes > 0:
                    status_msg += f" | Tempo restante: ~{estimated_minutes}min"
                else:
                    status_msg += f" | Tempo restante: ~{int(estimated_seconds)}s"
                
                status_text.success(status_msg)
            else:
                status_text.success(f"‚úÖ Lote {batch_num + 1}/{num_batches} conclu√≠do em {batch_time:.1f}s | ‚ö° {len(df_batch)/(batch_time+0.001):.1f} linhas/seg")
        
        total_time = time.time() - start_time
        total_minutes = int(total_time / 60)
        total_seconds = int(total_time % 60)
        avg_speed = len(all_results) / total_time
        
        progress_bar.empty()
        status_text.empty()
        batch_info.empty()
        
        st.success(f"üéâ **An√°lise completa em {total_minutes}min {total_seconds}s!** | ‚ö° Velocidade m√©dia: {avg_speed:.1f} linhas/seg")
        st.info(f"üìä Total de {len(all_results)} linhas processadas com sucesso")
        
        # Criar DataFrame e ordenar colunas
        df_results = pd.DataFrame(all_results)
        
        # Definir ordem correta das colunas
        colunas_ordenadas = [
            "Linha", "NR_OCORRENCIA", "Pedido", "Canal", "Tipo", "Tipo Manifesta√ß√£o", "Situa√ß√£o",
            "Pontua√ß√£o", "Classifica√ß√£o",
            "Score Frequ√™ncia", "Score Atraso", "Score Operacional", "Score Emocional",
            "Score Indicadores Externos", "Score Insatisfa√ß√£o Anterior", "Score Gravidade Canal", "Peso Base Canal",
            "Fatores Cr√≠ticos", "Amea√ßas Detectadas", "Tom Emocional", "Negativa T√©cnica?",
            "Padr√µes Comportamentais", "Canais de Escala√ß√£o", "Probabilidade Repetir", "Urg√™ncia",
            "Recomenda√ß√£o"
        ]
        
        # Reordenar apenas as colunas que existem
        colunas_existentes = [col for col in colunas_ordenadas if col in df_results.columns]
        df_results = df_results[colunas_existentes]
        
        return df_results
        
    except Exception as e:
        st.error(f"‚ùå Erro: {str(e)}")
        import traceback
        st.error(f"Detalhes: {traceback.format_exc()}")
        return None

# Interface principal
st.title("‚ö†Ô∏è An√°lise de Risco de Externaliza√ß√£o - Base Manifesta√ß√µes")
st.markdown("**Sistema com Metodologia SRO Dual Avan√ßada - ‚ö° VERS√ÉO OTIMIZADA**")
st.markdown("---")

st.markdown("""
### üìä Metodologia de An√°lise:

#### üü¢ **INTERNOS: 0-100 pontos** (Risco de virar externo)
- **0-74 pontos**: Classifica√ß√£o granular (0-5, 5-10... 70-74)
- **75-100 pontos**: üî¥ **RISCO ALTO DE EXTERNALIZAR**

#### üî¥ **EXTERNOS: 100-1000 pontos** (Risco de escala√ß√£o/repeti√ß√£o)
- **100-300**: ‚ö™ Baixo
- **301-500**: üü¢ M√©dio
- **501-700**: üü° Alto
- **701-850**: üü† Muito Alto
- **851-1000**: üî¥ **Vai Reclamar Novamente**

### ‚ö° **OTIMIZA√á√ïES APLICADAS:**
- üöÄ **Processamento Paralelo com Threads**: At√© 20 threads simult√¢neas
- üì¶ **Lotes maiores**: 100 linhas por lote (vs 50 anterior)
- ‚ö° **Modelo mais r√°pido**: GPT-4o-mini (melhor custo-benef√≠cio)
- üéØ **Tokens reduzidos**: Menos overhead, mesma qualidade
- üí® **Velocidade 3-5x maior** que a vers√£o anterior
- ‚úÖ **Sem depend√™ncias extras**: Funciona nativamente no Streamlit
""")

st.markdown("---")

# Upload
uploaded_file = st.file_uploader(
    "üìÅ Fa√ßa upload do Excel do dia (com planilha 'Base Manifesta√ß√µes')",
    type=['xlsx', 'xls'],
    help="Arquivo Excel contendo a planilha 'Base Manifesta√ß√µes'"
)

if uploaded_file is not None:
    st.success("‚úÖ Arquivo carregado!")
    
    if st.button("üöÄ Iniciar An√°lise OTIMIZADA (3-5x mais r√°pido)", type="primary", use_container_width=True):
        with st.spinner("‚ö° Iniciando an√°lise otimizada com processamento paralelo..."):
            results_df = process_excel_in_batches_optimized(uploaded_file, client)
        
        if results_df is not None and len(results_df) > 0:
            st.success("‚úÖ An√°lise completa conclu√≠da!")
            
            # Estat√≠sticas
            st.subheader("üìà Estat√≠sticas Gerais")
            
            internos = results_df[results_df["Tipo"] == "Interno"]
            externos = results_df[results_df["Tipo"] == "Externo"]
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Total de Casos", len(results_df))
            
            with col2:
                st.metric("Casos Internos", len(internos))
                if len(internos) > 0:
                    avg_int = internos["Pontua√ß√£o"].mean()
                    st.caption(f"M√©dia: {avg_int:.1f}/100")
            
            with col3:
                st.metric("Casos Externos", len(externos))
                if len(externos) > 0:
                    avg_ext = externos["Pontua√ß√£o"].mean()
                    st.caption(f"M√©dia: {avg_ext:.0f}/1000")
            
            with col4:
                criticos_int = len(internos[internos["Pontua√ß√£o"] >= 75]) if len(internos) > 0 else 0
                criticos_ext = len(externos[externos["Pontua√ß√£o"] >= 851]) if len(externos) > 0 else 0
                st.metric("Casos Cr√≠ticos", criticos_int + criticos_ext)
                st.caption(f"Int: {criticos_int} | Ext: {criticos_ext}")
            
            # Resultados
            st.subheader("üìã Resultados Completos")
            
            # Tabs para separar internos e externos
            tab1, tab2, tab3 = st.tabs(["üìä Todos", "üü¢ Internos", "üî¥ Externos"])
            
            with tab1:
                st.dataframe(results_df, use_container_width=True, height=400)
            
            with tab2:
                if len(internos) > 0:
                    st.dataframe(internos, use_container_width=True, height=400)
                else:
                    st.info("Nenhuma reclama√ß√£o interna encontrada")
            
            with tab3:
                if len(externos) > 0:
                    st.dataframe(externos, use_container_width=True, height=400)
                else:
                    st.info("Nenhuma reclama√ß√£o externa encontrada")
            
            # Download
            st.subheader("üíæ Download dos Resultados")
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_filename = f"analise_completa_sro_optimized_{timestamp}.xlsx"
            
            buffer = BytesIO()
            with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                results_df.to_excel(writer, index=False, sheet_name='An√°lise Completa')
                if len(internos) > 0:
                    internos.to_excel(writer, index=False, sheet_name='Internos')
                if len(externos) > 0:
                    externos.to_excel(writer, index=False, sheet_name='Externos')
            
            st.download_button(
                label="üì• Baixar Resultados Completos (Excel com 3 abas)",
                data=buffer.getvalue(),
                file_name=output_filename,
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True
            )
            
            # Casos priorit√°rios
            st.subheader("üö® Casos Priorit√°rios")
            
            priority_int = internos[internos["Pontua√ß√£o"] >= 75] if len(internos) > 0 else pd.DataFrame()
            priority_ext = externos[externos["Pontua√ß√£o"] >= 851] if len(externos) > 0 else pd.DataFrame()
            priority_cases = pd.concat([priority_int, priority_ext]) if len(priority_int) > 0 or len(priority_ext) > 0 else pd.DataFrame()
            
            if len(priority_cases) > 0:
                priority_cases = priority_cases.sort_values(by="Pontua√ß√£o", ascending=False)
                st.warning(f"‚ö†Ô∏è {len(priority_cases)} casos requerem aten√ß√£o priorit√°ria!")
                st.dataframe(
                    priority_cases[["Linha", "NR_OCORRENCIA", "Pedido", "Tipo", "Canal",
                                   "Pontua√ß√£o", "Classifica√ß√£o", "Recomenda√ß√£o"]],
                    use_container_width=True
                )
            else:
                st.success("‚úÖ Nenhum caso priorit√°rio identificado!")

else:
    st.info("üëÜ Fa√ßa upload de um arquivo Excel para come√ßar a an√°lise")

# Rodap√©
st.markdown("---")
st.markdown(f"""
<div style='text-align: center; color: #666; font-size: 0.9em;'>
    <p><strong>‚ö° An√°lise de Risco SRO Dual OTIMIZADA</strong> | Powered by OpenAI {MODEL}</p>
    <p>üìä Metodologia: INTERNOS (0-100 granular) | EXTERNOS (100-1000 com 5 n√≠veis)</p>
    <p>üöÄ Processamento paralelo: {BATCH_SIZE} linhas/lote | {MAX_WORKERS} threads simult√¢neas</p>
    <p>üí® <strong>Velocidade 3-5x maior</strong> que a vers√£o anterior</p>
    <p>‚úÖ <strong>Sem depend√™ncias extras</strong> - Funciona nativamente no Streamlit</p>
    <p>‚öôÔ∏è Configure OPENAI_API_KEY em Settings > Secrets</p>
</div>
""", unsafe_allow_html=True)
